{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brain Dump for Some Quick Image Processing Projects\n",
    "## 1. What's it all about anyway\n",
    "\n",
    "This is a dedicated repo to dump some quick image processing projects inspired by amazing [Adrian Rosebrock's](https://www.pyimagesearch.com/) blog. I'll be updating this repo as more things pile up. Feel free to use the code as a starting point for something more elaborate.<br>\n",
    "Here's a brief description of the available projects.\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "    <tr>\n",
    "        <th style=\"text-align:left\" width=\"33%\">Demo</th>\n",
    "        <th style=\"text-align:left\"            >Description</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td width=\"33%\">\n",
    "            <img align=left width=300 src=\"imgs/tentacle_beard.gif\"></img>\n",
    "        </td>\n",
    "        <td style=\"text-align:left\"> \n",
    "            <b>Tentacle Beard!</b>\n",
    "            <br>\n",
    "            <br>         \n",
    "            Everyone is better off with a tentacle beard!<br>\n",
    "            Run `tentacle_beard.py` to capture a video stream from your machine's camera \n",
    "            and give any human-ish individual caught by the camera a fresh new look!\n",
    "            <br>       \n",
    "            <ul>\n",
    "                <li>Most heavy-lifting is done by the \n",
    "                    <a href=\"https://www.pyimagesearch.com/2017/04/03/facial-landmarks-dlib-opencv-python/\">facial landmark</a> detector \n",
    "                    conveniently available out-of-the-box from <a href=\"https://pypi.org/project/dlib/\"> dlib</a>.</li>\n",
    "                <li> <a href=\"https://pypi.org/project/dlib/\"> Dlib</a>'s facial landmark detector is based on <a href=\"https://lear.inrialpes.fr/people/triggs/pubs/Dalal-cvpr05.pdf\"> Histogram </a> \n",
    "                    <a href=\"https://www.learnopencv.com/histogram-of-oriented-gradients/\"> \n",
    "                        of Oriented Gradients</a> (HOG) detector and a \n",
    "                    linear <a href=\"https://en.wikipedia.org/wiki/Support-vector_machine\">SVM</a> classifier.                    \n",
    "                </li>\n",
    "                <li>Facial landmarks with indices 3-15 \n",
    "                    (<a href=\"https://www.researchgate.net/figure/The-68-points-mark-up-used-for-our-annotations-32_fig2_313584884\">img</a>) \n",
    "                    serve as the anchor points for 13 beard tentacles<br>\n",
    "                </li>                    \n",
    "                <li>Tentacles are animated \n",
    "                    through the magic of sine-waves and smooth \"randomness\" of \n",
    "                    <a href=\"https://en.wikipedia.org/wiki/Perlin_noise\"> Perlin noise</a>:\n",
    "                    <br><br>\n",
    "                    <ul style=\"list-style-type:circle;\">\n",
    "                        <li>Each tentacle is composed of a series of straight segments \n",
    "                            with exponentially decaying length;\n",
    "                        </li>\n",
    "                        <li>\n",
    "                            For each video frame the angles between the consecutive segments \n",
    "                            follow simple sine rule;\n",
    "                        </li>\n",
    "                        <li>\n",
    "                            Amplitude, frequency and phase shift of the sine wave \n",
    "                            are sampled from the 2D Perlin matrix;\n",
    "                        </li>\n",
    "                        <li>\n",
    "                            Each tentacle has a dedicated row in the Perlin matrix;\n",
    "                            and for each new video frame the `next` element of the row is selected\n",
    "                        </li>\n",
    "                    </ul>\n",
    "                </li>\n",
    "            </ul>  \n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td width=\"33%\"> \n",
    "            <img align=left width=300 src=\"imgs/hand_controls.gif\"></img> \n",
    "        </td>\n",
    "        <td style=\"text-align:left\"> \n",
    "            <b>Hand Controls</b>\n",
    "            <br><br>           \n",
    "            Ever felt like controlling the video settings of your camera \n",
    "            without actually touching the keyboard?<br>\n",
    "            Run `hand_controls.py` and toggle custom video switches \n",
    "            by waving hands around the control buttons!<br>\n",
    "            So far I've only added control buttons to resize the video frame \n",
    "            and to blur out the faces <br>\n",
    "           (who knows, might come in handy...).\n",
    "            <br>         \n",
    "            <ul>\n",
    "                <li>The most compute-heavy part of this project is the hand detector \n",
    "                    (`dnn/frozen_inference_graph_for_hand_detection.pb`): \n",
    "                </li>\n",
    "                    <ul style=\"list-style-type:circle;\">\n",
    "                        <li>\n",
    "                            Base architecture: <a href=\"https://arxiv.org/abs/1512.02325\">SSD</a> \n",
    "                            <a href=\"https://arxiv.org/abs/1704.04861\">MobileNet V1</a> trained on \n",
    "                            <a href=\"http://cocodataset.org/#home\">COCO dataset</a> \n",
    "                            available out-of-the-box from \n",
    "                            <a href=\"https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md\">tensorflow</a>;                            \n",
    "                        </li>\n",
    "                        <li>\n",
    "                            Assembled with \n",
    "                            <a href=\"https://github.com/tensorflow/models/tree/master/research/object_detection\">Tensorflow Object Detection API</a> \n",
    "                            and trained on \n",
    "                            <a href=\"http://vision.soic.indiana.edu/projects/egohands/\">Egohands dataset</a>;\n",
    "                        </li>\n",
    "                    </ul>\n",
    "                <li>Each video frame is fed to the hand detector, \n",
    "                    which in turn produces the bounding boxes for the hands\n",
    "                    (if present)\n",
    "                </li>\n",
    "                <li>\n",
    "                    If the bounding box of a hand overlaps with the area taken by any of the control buttons,\n",
    "                    the action corresponding to that button \n",
    "                    (e.g. resize video frame or blur all faces)\n",
    "                    if executed                    \n",
    "                </li>\n",
    "                <li>\n",
    "                    For face detection (needed for bluring the corresponding area)\n",
    "                    I use simple <a href=\"https://docs.opencv.org/3.4.1/d7/d8b/tutorial_py_face_detection.html\">HAAR cascade</a>, \n",
    "                    available in <a href=\"https://github.com/opencv/opencv/tree/master/data/haarcascades\">OpenCV</a>.\n",
    "                </li>\n",
    "            </ul>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "## 2. Requirements\n",
    "Your device should have a functioning camera and [python 3](https://www.python.org/download/releases/3.0/) installed. </br>\n",
    "Ideally you'd want to run this (any) project from a [docker](https://www.docker.com/) container (see below the instructions for setting up a container). But if you're feeling adventurous, you can take care of all the dependencies manually.\n",
    "\n",
    "### Dependencies\n",
    "```\n",
    "$ pip3 install imutils\n",
    "$ pip3 install argparse\n",
    "$ pip3 install matplotlib\n",
    "$ pip3 install opencv-python\n",
    "$ pip3 install dlib\n",
    "$ pip3 install tensorflow==1.14.0\n",
    "$ pip3 install numpy==1.14.0\n",
    "```\n",
    "\n",
    "To install `dlib` on Windows use this wheel:\n",
    "```\n",
    "pip install https://pypi.python.org/packages/da/06/bd3e241c4eb0a662914b3b4875fc52dd176a9db0d4a2c915ac2ad8800e9e/dlib-19.7.0-cp36-cp36m-win_amd64.whl#md5=b7330a5b2d46420343fbed5df69e6a3f\n",
    "```\n",
    "\n",
    "Take care to install `tensorflow` version 1.14.0 or above, as the inference graph for the hand detector in `hand_controls.py` will give errors with the earlier versions of tensorflow.\n",
    "\n",
    "Downgrading `numpy` to version 1.14.0 is not really necessary (yet!), but it will save you a headache of scrolling through the deprecation warnings.\n",
    "\n",
    "### Dependencies of dependencies\n",
    "To compile `dlib` you need to have [cmake](https://cmake.org/) and C++ compiler.</br>\n",
    "On Linux both are covered by:\n",
    "```\n",
    "$ sudo apt-get update && sudo apt-get cmake\n",
    "```\n",
    "On Windows you can get `cmake` from https://cmake.org/download/ and `gcc` compiler for C++ from https://osdn.net/projects/mingw/releases/.\n",
    "\n",
    "To use `OpenCV` on Linux you might need to have the following installed:\n",
    "```\n",
    "$ sudo apt-get install -y libsm6 libxext6 libxrender-dev\n",
    "```\n",
    "\n",
    "## 3. Installation\n",
    "Copy this repo to your device by \n",
    "```\n",
    "$ git clone https://github.com/axyorah/quick_img_processing.git\n",
    "```\n",
    "\n",
    "With all the dependencies taken care of, you can tentaclify yourself by navigating to the directory, to which you have copied the contents of this repo, and running:\n",
    "```\n",
    "$ python3 tentacle_beard.py\n",
    "```\n",
    "\n",
    "You can regulate the degree of \"wiggliness\" of the tentacles through `-w` parameter, e.g.:\n",
    "```\n",
    "$ python3 tentacle_beard.py -w 0.3\n",
    "```\n",
    "\n",
    "Similarly, you can invoke the power of hand controls by running:\n",
    "\n",
    "```\n",
    "$ python3 hand_controls.py\n",
    "```\n",
    "\n",
    "Passing `-b 1` parameter will additionally draw bounding boxes around the hands (just like in the demo):\n",
    "\n",
    "```\n",
    "$ python3 hand_controls.py -b 1\n",
    "```\n",
    "\n",
    "\n",
    "### Docker\n",
    "You can set up a [docker](https://www.docker.com/) container to take care of the environment. </br>\n",
    "If you don't have `docker`, follow the [instructions](https://docs.docker.com/install/) for your system to install it and add yourself to `docker` group.</br>\n",
    "With `docker` installed, build `docker` image by running the following command from the directory, where you have copied the contents of this repo:\n",
    "```\n",
    "$ docker build -t quick_img_processing .\n",
    "```\n",
    "\n",
    "Set up `docker` container by running the following command:\n",
    "```\n",
    "$ docker run --rm -it \\\n",
    "  --device=/dev/video0 \\\n",
    "  -v /tmp/.X11-unix:/tmp/.X11-unix \\\n",
    "  -v $XAUTHORITY:/root/.Xauthority \\\n",
    "  -e DISPLAY=$DISPLAY \\\n",
    "  quick_img_processing\n",
    "```\n",
    "\n",
    "This should take care of accessing your machine's camera from the container and displaying the video output. If you're still getting X server related errors, check these resources:\n",
    "- https://github.com/opencv/gst-video-analytics/wiki/Docker-Run\n",
    "- https://towardsdatascience.com/real-time-and-video-processing-object-detection-using-tensorflow-opencv-and-docker-2be1694726e5\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
